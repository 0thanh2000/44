{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0thanh2000/44/blob/main/11a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_RksYA-sxCO"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 0. MOUNT DRIVE VÀ IMPORT THƯ VIỆN\n",
        "# -----------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWA_8VPBsxCP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, logging\n",
        "\n",
        "from datetime import timedelta\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Dropout, LayerNormalization, MultiHeadAttention, Add, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# -----------------------------\n",
        "# 1. THIẾT LẬP THAM SỐ & ĐỌC DỮ LIỆU\n",
        "# -----------------------------\n",
        "# Các tham số chính:\n",
        "window_m5   = 144    # Số nến M5\n",
        "window_m30  = 120    # Số nến M30\n",
        "window_h2   = 200    # Số nến H2\n",
        "num_features = 4     # Số đặc trưng: Open, High, Low, Close\n",
        "dtype = np.float32   # Kiểu dữ liệu sử dụng\n",
        "batch_size = 32      # Kích thước batch\n",
        "\n",
        "# Upload file CSV (nhớ upload file lên Colab trước khi chạy)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Đọc dữ liệu từ các file CSV\n",
        "df_m5  = pd.read_csv('GBPUSD_M5a.csv', parse_dates=['Datetime'])\n",
        "df_m30 = pd.read_csv('GBPUSD_M30a.csv', parse_dates=['Datetime'])\n",
        "df_h2  = pd.read_csv('GBPUSD_H2a.csv', parse_dates=['Datetime'])\n",
        "\n",
        "# Sắp xếp theo thời gian, reset index và loại bỏ giá trị thiếu\n",
        "for df in [df_m5, df_m30, df_h2]:\n",
        "    df.sort_values('Datetime', inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r88Pukc8sxCP"
      },
      "outputs": [],
      "source": [
        "# 2. XỬ LÝ DỮ LIỆU: TẠO TARGET INDICES VÀ CHIA TRAIN-TEST\n",
        "# -----------------------------\n",
        "# Hàm lấy lịch sử dữ liệu của từng khung thời gian (dùng cho cả training và backtest)\n",
        "def get_history(target_time, df, window):\n",
        "    mask = df['Datetime'] < target_time\n",
        "    if mask.sum() < window:\n",
        "        return None\n",
        "    return df.loc[mask].tail(window)[['Open', 'High', 'Low', 'Close']].values\n",
        "\n",
        "# Tạo danh sách các index hợp lệ từ dữ liệu H2\n",
        "valid_indices = []\n",
        "for i in range(window_h2, len(df_h2) - 1):\n",
        "    target_time = df_h2.iloc[i+1]['Datetime']\n",
        "    if (get_history(target_time, df_m5, window_m5) is not None and\n",
        "        get_history(target_time, df_m30, window_m30) is not None):\n",
        "        valid_indices.append(i)\n",
        "\n",
        "# Chia train-test theo thời gian (25% đầu làm test, phần còn lại làm train)\n",
        "split_idx = int(len(valid_indices) * 0.25)\n",
        "test_indices = valid_indices[:split_idx]\n",
        "train_indices = valid_indices[split_idx:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFtGjJ0rsxCQ"
      },
      "outputs": [],
      "source": [
        "# 3. KHỞI TẠO SCALER SỬ DỤNG TẬP TRAIN\n",
        "# -----------------------------\n",
        "def init_scalers(indices):\n",
        "    m5_data, m30_data, h2_data, y_data = [], [], [], []\n",
        "    for idx in indices:\n",
        "        target_time = df_h2.iloc[idx+1]['Datetime']\n",
        "        m5   = get_history(target_time, df_m5, window_m5)\n",
        "        m30  = get_history(target_time, df_m30, window_m30)\n",
        "        h2   = df_h2.iloc[idx-window_h2+1:idx+1][['Open', 'High', 'Low', 'Close']].values\n",
        "        y    = df_h2.iloc[idx+1]['Close']\n",
        "\n",
        "        m5_data.append(m5)\n",
        "        m30_data.append(m30)\n",
        "        h2_data.append(h2)\n",
        "        y_data.append(y)\n",
        "\n",
        "    scaler_m5 = RobustScaler()\n",
        "    scaler_m5.fit(np.vstack(m5_data).reshape(-1, num_features))\n",
        "\n",
        "    scaler_m30 = RobustScaler()\n",
        "    scaler_m30.fit(np.vstack(m30_data).reshape(-1, num_features))\n",
        "\n",
        "    scaler_h2 = RobustScaler()\n",
        "    scaler_h2.fit(np.vstack(h2_data).reshape(-1, num_features))\n",
        "\n",
        "    scaler_y = RobustScaler()\n",
        "    scaler_y.fit(np.array(y_data).reshape(-1, 1))\n",
        "\n",
        "    return scaler_m5, scaler_m30, scaler_h2, scaler_y\n",
        "\n",
        "scaler_m5, scaler_m30, scaler_h2, scaler_y = init_scalers(train_indices)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERMWUfmisxCQ"
      },
      "outputs": [],
      "source": [
        "# 4. XÂY DỰNG tf.data.Dataset TỪ GENERATOR (KHÔNG SHUFFLE)\n",
        "# -----------------------------\n",
        "def sample_generator(indices):\n",
        "    \"\"\"\n",
        "    Generator trả về từng sample với input là dict chứa dữ liệu của các khung (M5, M30, H2)\n",
        "    và target là giá (đã được chuẩn hóa).\n",
        "    \"\"\"\n",
        "    for idx in indices:\n",
        "        target_time = df_h2.iloc[idx+1]['Datetime']\n",
        "        m5 = get_history(target_time, df_m5, window_m5)\n",
        "        m30 = get_history(target_time, df_m30, window_m30)\n",
        "        h2 = df_h2.iloc[idx-window_h2+1:idx+1][['Open', 'High', 'Low', 'Close']].values\n",
        "        y  = df_h2.iloc[idx+1]['Close']\n",
        "\n",
        "        # Chuẩn hóa và chuyển sang kiểu dữ liệu đã định nghĩa\n",
        "        m5_scaled   = scaler_m5.transform(m5.reshape(-1, num_features)).reshape(m5.shape).astype(dtype)\n",
        "        m30_scaled  = scaler_m30.transform(m30.reshape(-1, num_features)).reshape(m30.shape).astype(dtype)\n",
        "        h2_scaled   = scaler_h2.transform(h2.reshape(-1, num_features)).reshape(h2.shape).astype(dtype)\n",
        "        y_scaled    = scaler_y.transform([[y]]).flatten().astype(dtype)\n",
        "\n",
        "        yield (\n",
        "            {\n",
        "                'input_m5': m5_scaled,\n",
        "                'input_m30': m30_scaled,\n",
        "                'input_h2': h2_scaled\n",
        "            },\n",
        "            y_scaled\n",
        "        )\n",
        "\n",
        "# Định nghĩa cấu trúc đầu ra cho tf.data.Dataset\n",
        "output_signature = (\n",
        "    {\n",
        "        'input_m5': tf.TensorSpec(shape=(window_m5, num_features), dtype=tf.float32),\n",
        "        'input_m30': tf.TensorSpec(shape=(window_m30, num_features), dtype=tf.float32),\n",
        "        'input_h2': tf.TensorSpec(shape=(window_h2, num_features), dtype=tf.float32)\n",
        "    },\n",
        "    tf.TensorSpec(shape=(), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "# Tạo Dataset cho tập train và test (không dùng shuffle, giữ nguyên thứ tự)\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: sample_generator(train_indices),\n",
        "                                                 output_signature=output_signature)\n",
        "test_dataset = tf.data.Dataset.from_generator(lambda: sample_generator(test_indices),\n",
        "                                                output_signature=output_signature)\n",
        "\n",
        "# Áp dụng batch và prefetch để tối ưu quá trình huấn luyện\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwkOOvRUsxCR"
      },
      "outputs": [],
      "source": [
        "# 5. XÂY DỰNG MÔ HÌNH (Multi-branch với LSTM + MultiHeadAttention)\n",
        "# -----------------------------\n",
        "def build_branch(input_shape, name_prefix):\n",
        "    input_layer = Input(shape=input_shape, name=f'input_{name_prefix}', dtype=dtype)\n",
        "    x = LSTM(64, return_sequences=True, name=f'lstm_{name_prefix}')(input_layer)\n",
        "    x_attn = MultiHeadAttention(num_heads=4, key_dim=16, name=f'mha_{name_prefix}')(x, x)\n",
        "    x = Add(name=f'add_{name_prefix}')([x, x_attn])\n",
        "    x = LayerNormalization(name=f'ln_{name_prefix}')(x)\n",
        "    # Global Average Pooling theo chiều thời gian\n",
        "    x = Lambda(lambda t: tf.reduce_mean(t, axis=1), name=f'global_avg_{name_prefix}')(x)\n",
        "    x = Dropout(0.01, name=f'dropout_{name_prefix}')(x)\n",
        "    return input_layer, x\n",
        "\n",
        "# Xây dựng các nhánh cho M5, M30 và H2\n",
        "input_m5_layer, branch_m5 = build_branch((window_m5, num_features), 'm5')\n",
        "input_m30_layer, branch_m30 = build_branch((window_m30, num_features), 'm30')\n",
        "input_h2_layer, branch_h2 = build_branch((window_h2, num_features), 'h2')\n",
        "\n",
        "# Fusion Attention: hợp nhất các đặc trưng từ 3 nhánh\n",
        "combined = Concatenate(name='concatenate')([branch_m5, branch_m30, branch_h2])\n",
        "fusion_query = Lambda(lambda t: tf.expand_dims(t, axis=1), name='fusion_expand_dims')(combined)\n",
        "fusion_attn = MultiHeadAttention(num_heads=4, key_dim=16, name='fusion_mha')(fusion_query, fusion_query)\n",
        "fusion_attn = Lambda(lambda t: tf.squeeze(t, axis=1), name='fusion_squeeze')(fusion_attn)\n",
        "fusion_out = Add(name='fusion_add')([combined, fusion_attn])\n",
        "fusion_out = LayerNormalization(name='fusion_ln')(fusion_out)\n",
        "fusion_out = Dropout(0.01, name='fusion_dropout')(fusion_out)\n",
        "\n",
        "# Các lớp Dense cuối cùng\n",
        "x = Dense(64, activation='relu', name='dense_1')(fusion_out)\n",
        "x = LayerNormalization(name='dense_ln')(x)\n",
        "x = Dropout(0.01, name='dense_dropout')(x)\n",
        "output = Dense(1, activation='linear', name='output')(x)\n",
        "\n",
        "# Xây dựng mô hình\n",
        "model = Model(inputs=[input_m5_layer, input_m30_layer, input_h2_layer], outputs=output)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "# Sử dụng Huber Loss – ổn định với dữ liệu nhiễu\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='mae',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDpMs0QwsxCR"
      },
      "outputs": [],
      "source": [
        "# 6. HUẤN LUYỆN MÔ HÌNH VỚI tf.data.Dataset\n",
        "# -----------------------------\n",
        "# Cài đặt callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=8, min_delta=0, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/7aEUa.keras', monitor='val_loss', save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-8, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjyvOkxYsxCR"
      },
      "outputs": [],
      "source": [
        "# 7. LƯU SCALERS VÀ MODEL\n",
        "# -----------------------------\n",
        "joblib.dump(scaler_m5, '/content/drive/MyDrive/scaler_m5.joblib')\n",
        "joblib.dump(scaler_m30, '/content/drive/MyDrive/scaler_m30.joblib')\n",
        "joblib.dump(scaler_h2, '/content/drive/MyDrive/scaler_h2.joblib')\n",
        "joblib.dump(scaler_y, '/content/drive/MyDrive/scaler_y.joblib')\n",
        "model.save('/content/drive/MyDrive/full_model.keras')\n",
        "\n",
        "# Giải phóng bộ nhớ\n",
        "del df_m5, df_m30, df_h2\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY94unTlsxCR"
      },
      "outputs": [],
      "source": [
        "# 8. ĐÁNH GIÁ VÀ TRỰC QUAN HÓA KẾT QUẢ HUẤN LUYỆN\n",
        "# -----------------------------\n",
        "# Lấy dự báo từ test_dataset\n",
        "y_pred = model.predict(test_dataset)\n",
        "y_pred = y_pred.flatten()\n",
        "\n",
        "# Vì dữ liệu target trong generator đã được chuẩn hóa, thực hiện inverse transform:\n",
        "y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Tạo mảng ground truth (y_test) từ test_indices\n",
        "# Lấy giá từ df_h2 (lưu ý: nếu bạn đã xóa df_h2, bạn cần lưu lại giá trị gốc trước)\n",
        "# Ở đây, ta tái tạo y_test từ test_indices (giả sử bạn vẫn có file gốc)\n",
        "df_h2_raw = pd.read_csv('GBPUSD_H2a.csv', parse_dates=['Datetime'])\n",
        "df_h2_raw.sort_values('Datetime', inplace=True)\n",
        "df_h2_raw.reset_index(drop=True, inplace=True)\n",
        "df_h2_raw.dropna(inplace=True)\n",
        "\n",
        "y_test_list = []\n",
        "for idx in test_indices:\n",
        "    y_val = df_h2_raw.iloc[idx+1]['Close']\n",
        "    y_test_list.append(y_val)\n",
        "y_test_array = np.array(y_test_list)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score, median_absolute_error\n",
        "mae   = mean_absolute_error(y_test_array, y_pred)\n",
        "mse   = mean_squared_error(y_test_array, y_pred)\n",
        "mape  = mean_absolute_percentage_error(y_test_array, y_pred)\n",
        "r2    = r2_score(y_test_array, y_pred)\n",
        "medae = median_absolute_error(y_test_array, y_pred)\n",
        "\n",
        "# Tính Direction Accuracy: tỷ lệ dự báo đúng hướng thay đổi của giá\n",
        "actual_diff = np.diff(y_test_array)\n",
        "pred_diff   = np.diff(y_pred)\n",
        "direction_accuracy = np.mean(np.sign(actual_diff) == np.sign(pred_diff))\n",
        "\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
        "print(\"R² Score:\", r2)\n",
        "print(\"Median Absolute Error:\", medae)\n",
        "print(\"Direction Accuracy:\", direction_accuracy)\n",
        "\n",
        "# Vẽ biểu đồ Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Vẽ biểu đồ so sánh giá thực tế và giá dự báo\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_array, label=\"Giá thực tế\", color=\"blue\", linestyle=\"-\", marker=\"o\")\n",
        "plt.plot(y_pred, label=\"Giá dự báo\", color=\"red\", linestyle=\"--\", marker=\"x\")\n",
        "plt.xlabel(\"Chỉ số mẫu\")\n",
        "plt.ylabel(\"Giá\")\n",
        "plt.title(\"So sánh Giá thực tế và Giá dự báo\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f80NB6osxCS"
      },
      "outputs": [],
      "source": [
        "# 9. PHẦN BACKTEST (TUỲ CHỌN)\n",
        "# -----------------------------\n",
        "# Cấu hình logging cho backtest\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='[%(asctime)s] %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "def read_data(csv_path):\n",
        "    try:\n",
        "        logging.info(\"Đọc dữ liệu từ file CSV: %s\", csv_path)\n",
        "        df = pd.read_csv(csv_path, parse_dates=[\"Datetime\"])\n",
        "    except Exception as e:\n",
        "        logging.error(\"Không thể đọc file CSV: %s\", e)\n",
        "        sys.exit(1)\n",
        "    df.sort_values(\"Datetime\", inplace=True)\n",
        "    required_cols = [\"Datetime\", \"Open\", \"High\", \"Low\", \"Close\"]\n",
        "    for col in required_cols:\n",
        "        if col not in df.columns:\n",
        "            logging.error(\"File CSV thiếu cột bắt buộc: %s\", col)\n",
        "            sys.exit(1)\n",
        "    if \"Volume\" in df.columns:\n",
        "        logging.info(\"Loại bỏ cột Volume khỏi dữ liệu.\")\n",
        "    df = df[required_cols]\n",
        "    return df\n",
        "\n",
        "def get_history_backtest(target_time, df, window):\n",
        "    df_hist = df[df['Datetime'] < target_time].tail(window)\n",
        "    if len(df_hist) == window:\n",
        "        return df_hist[['Open', 'High', 'Low', 'Close']].values\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def create_sequences_from_df(df, window_m5, window_m30, window_h2, horizon):\n",
        "    X_m5, X_m30, X_h2, Y, time_list = [], [], [], [], []\n",
        "    end_index = len(df) - horizon + 1\n",
        "    for i in range(window_h2, end_index):\n",
        "        target_seq = df.loc[i : i+horizon-1, \"Close\"].values\n",
        "        target_time = df.loc[i+horizon-1, \"Datetime\"]\n",
        "        seq_m5 = get_history_backtest(target_time, df, window_m5)\n",
        "        seq_m30 = get_history_backtest(target_time, df, window_m30)\n",
        "        seq_h2 = df.loc[i-window_h2+1 : i, [\"Open\", \"High\", \"Low\", \"Close\"]].values\n",
        "        if (seq_m5 is not None) and (seq_m30 is not None) and (len(seq_h2)==window_h2):\n",
        "            X_m5.append(seq_m5)\n",
        "            X_m30.append(seq_m30)\n",
        "            X_h2.append(seq_h2)\n",
        "            Y.append(target_seq)\n",
        "            time_list.append(target_time)\n",
        "    return np.array(X_m5), np.array(X_m30), np.array(X_h2), np.array(Y), np.array(time_list)\n",
        "\n",
        "def transform_3d_array(X, scaler):\n",
        "    shape = X.shape\n",
        "    X_reshaped = X.reshape(-1, shape[-1])\n",
        "    X_scaled = scaler.transform(X_reshaped).reshape(shape)\n",
        "    return X_scaled\n",
        "\n",
        "def compute_directional_accuracy(y_true, y_pred):\n",
        "    diff_true = np.diff(y_true[:, -1], axis=0)\n",
        "    diff_pred = np.diff(y_pred[:, -1], axis=0)\n",
        "    correct = np.sum(np.sign(diff_true) == np.sign(diff_pred))\n",
        "    return correct / len(diff_true)\n",
        "\n",
        "def save_predictions_to_csv(dates, actual, predicted, output_path):\n",
        "    n, horizon = actual.shape\n",
        "    columns = [\"Datetime\"] + [f\"Actual_{i+1}\" for i in range(horizon)] + [f\"Predicted_{i+1}\" for i in range(horizon)]\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        row = [dates[i]] + list(actual[i]) + list(predicted[i])\n",
        "        data.append(row)\n",
        "    df_out = pd.DataFrame(data, columns=columns)\n",
        "    try:\n",
        "        df_out.to_csv(output_path, index=False)\n",
        "        logging.info(\"Đã lưu kết quả dự báo vào file: %s\", output_path)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Lỗi khi lưu kết quả dự báo: %s\", e)\n",
        "\n",
        "def backtest():\n",
        "    # Cấu hình đường dẫn cho file backtest và các scaler, model đã lưu\n",
        "    csv_path = \"/content/drive/MyDrive/UJ_M15u_backtest.csv\"\n",
        "    scaler_m5_path = \"/content/drive/MyDrive/scaler_m5.save\"\n",
        "    scaler_m30_path = \"/content/drive/MyDrive/scaler_m30.save\"\n",
        "    scaler_h2_path = \"/content/drive/MyDrive/scaler_h2.save\"\n",
        "    scaler_y_path = \"/content/drive/MyDrive/scaler_y.save\"\n",
        "    model_path = \"/content/drive/MyDrive/UJ_finam288_1.keras\"\n",
        "    output_csv = \"/content/drive/MyDrive/backtest_results.csv\"\n",
        "\n",
        "    window_m5 = 144\n",
        "    window_m30 = 120\n",
        "    window_h2 = 200\n",
        "    horizon = 8\n",
        "\n",
        "    df = read_data(csv_path)\n",
        "    try:\n",
        "        logging.info(\"Trích xuất features và target từ dữ liệu backtest.\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Lỗi khi trích xuất features: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        from joblib import load\n",
        "        logging.info(\"Loading scaler M5 từ: %s\", scaler_m5_path)\n",
        "        scaler_m5 = load(scaler_m5_path)\n",
        "        logging.info(\"Loading scaler M30 từ: %s\", scaler_m30_path)\n",
        "        scaler_m30 = load(scaler_m30_path)\n",
        "        logging.info(\"Loading scaler H2 từ: %s\", scaler_h2_path)\n",
        "        scaler_h2 = load(scaler_h2_path)\n",
        "        logging.info(\"Loading scaler y từ: %s\", scaler_y_path)\n",
        "        scaler_y = load(scaler_y_path)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Lỗi khi load scaler: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    X_m5, X_m30, X_h2, y_seq, time_seq = create_sequences_from_df(df, window_m5, window_m30, window_h2, horizon)\n",
        "    logging.info(\"X_m5.shape = %s\", X_m5.shape)\n",
        "    logging.info(\"X_m30.shape = %s\", X_m30.shape)\n",
        "    logging.info(\"X_h2.shape = %s\", X_h2.shape)\n",
        "    logging.info(\"y_seq.shape = %s\", y_seq.shape)\n",
        "\n",
        "    X_m5_scaled = transform_3d_array(X_m5, scaler_m5)\n",
        "    X_m30_scaled = transform_3d_array(X_m30, scaler_m30)\n",
        "    X_h2_scaled = transform_3d_array(X_h2, scaler_h2)\n",
        "    y_seq_scaled = scaler_y.transform(y_seq.reshape(-1, 1)).reshape(y_seq.shape)\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Loading mô hình từ: %s\", model_path)\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Lỗi khi load mô hình: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        predictions_scaled = model.predict([X_m5_scaled, X_m30_scaled, X_h2_scaled])\n",
        "        logging.info(\"predictions_scaled.shape = %s\", predictions_scaled.shape)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Lỗi khi chạy dự đoán: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        y_inversed = scaler_y.inverse_transform(y_seq_scaled.reshape(-1, 1)).reshape(y_seq_scaled.shape)\n",
        "        pred_inversed = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).reshape(predictions_scaled.shape)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Lỗi khi inverse transform: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score, median_absolute_error\n",
        "    mae = mean_absolute_error(y_inversed, pred_inversed)\n",
        "    rmse = np.sqrt(mean_squared_error(y_inversed, pred_inversed))\n",
        "    mape_value = mean_absolute_percentage_error(y_inversed, pred_inversed) * 100\n",
        "    r2 = r2_score(y_inversed, pred_inversed)\n",
        "    medae = median_absolute_error(y_inversed, pred_inversed)\n",
        "    dir_acc = compute_directional_accuracy(y_inversed, pred_inversed)\n",
        "\n",
        "    logging.info(\"MAE (horizon %d): %.4f\", horizon, mae)\n",
        "    logging.info(\"RMSE (horizon %d): %.4f\", horizon, rmse)\n",
        "    logging.info(\"MAPE: %.2f%%\", mape_value)\n",
        "    logging.info(\"R²: %.4f\", r2)\n",
        "    logging.info(\"Median Absolute Error: %.4f\", medae)\n",
        "    logging.info(\"Directional Accuracy (bước cuối horizon): %.2f%%\", dir_acc * 100)\n",
        "\n",
        "    print(\"MAE (horizon {}): {:.4f}\".format(horizon, mae))\n",
        "    print(\"RMSE (horizon {}): {:.4f}\".format(horizon, rmse))\n",
        "    print(\"MAPE: {:.2f}%\".format(mape_value))\n",
        "    print(\"R²: {:.4f}\".format(r2))\n",
        "    print(\"Median Absolute Error: {:.4f}\".format(medae))\n",
        "    print(\"Directional Accuracy (bước cuối horizon): {:.2f}%\".format(dir_acc * 100))\n",
        "\n",
        "    if time_seq is not None:\n",
        "        save_predictions_to_csv(time_seq, y_inversed, pred_inversed, output_csv)\n",
        "    else:\n",
        "        n_samples = y_inversed.shape[0]\n",
        "        df_pred = pd.DataFrame({\"Index\": np.arange(n_samples)})\n",
        "        for i in range(horizon):\n",
        "            df_pred[f\"Actual_{i+1}\"] = y_inversed[:, i]\n",
        "            df_pred[f\"Predicted_{i+1}\"] = pred_inversed[:, i]\n",
        "        try:\n",
        "            df_pred.to_csv(output_csv, index=False)\n",
        "            logging.info(\"Đã lưu kết quả dự báo vào file: %s\", output_csv)\n",
        "        except Exception as e:\n",
        "            logging.error(\"Lỗi khi lưu kết quả dự báo: %s\", e)\n",
        "\n",
        "    plt.figure(figsize=(14,7))\n",
        "    if time_seq is not None:\n",
        "        plt.plot(time_seq, y_inversed[:, -1], label=\"Giá thực tế (bước cuối)\", color=\"blue\")\n",
        "        plt.plot(time_seq, pred_inversed[:, -1], label=\"Dự báo (bước cuối)\", linestyle=\"--\", color=\"red\")\n",
        "        plt.xlabel(\"Datetime\")\n",
        "    else:\n",
        "        plt.plot(y_inversed[:, -1], label=\"Giá thực tế (bước cuối)\", color=\"blue\")\n",
        "        plt.plot(pred_inversed[:, -1], label=\"Dự báo (bước cuối)\", linestyle=\"--\", color=\"red\")\n",
        "        plt.xlabel(\"Index\")\n",
        "    plt.ylabel(\"Giá\")\n",
        "    plt.title(\"So sánh Giá thực tế và Giá dự báo (bước cuối của horizon)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Để chạy phần backtest, chỉ cần gọi:\n",
        "# backtest()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}